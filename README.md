# ğŸ§  SystÃ¨me RAG (Retrieval-Augmented Generation)

## Vue d'ensemble

Ce systÃ¨me RAG permet d'interroger des documents (PDF, texte) via une interface de chat. Il combine la recherche d'information avec la gÃ©nÃ©ration de rÃ©ponses par un modÃ¨le de langage (LLM).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ARCHITECTURE RAG                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   ğŸ“„ Documents                                                              â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â–¼                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚   ROUTER    â”‚â”€â”€â”€â”€â–¶â”‚  Pipeline Vision (LlamaParse)                â”‚     â”‚
â”‚   â”‚  (Gardien)  â”‚     â”‚  - OCR avancÃ©, tables, images                â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â–¼                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  Pipeline Texte (PyPDF)                                          â”‚     â”‚
â”‚   â”‚  - Extraction rapide, chunking rÃ©cursif                          â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â–¼                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚              INDEXATION PARENT-CHILD                             â”‚     â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚     â”‚
â”‚   â”‚  â”‚   DocStore      â”‚       â”‚   VectorStore   â”‚                   â”‚     â”‚
â”‚   â”‚  â”‚   (Parents)     â”‚â—„â”€â”€â”€â”€â”€â–¶â”‚   (Enfants)     â”‚                   â”‚     â”‚
â”‚   â”‚  â”‚   Docs complets â”‚       â”‚   Petits chunks â”‚                   â”‚     â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                             â”‚
â”‚                            RECHERCHE                                        â”‚
â”‚                               â”‚                                             â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚        â–¼                      â–¼                      â–¼                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚  BM25   â”‚          â”‚  Vectoriel  â”‚        â”‚ Reranker â”‚                 â”‚
â”‚   â”‚ (Mots)  â”‚          â”‚ (SÃ©mantique)â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   BGE    â”‚                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚        â”‚                      â”‚                      â”‚                     â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                               â”‚                                             â”‚
â”‚                               â–¼                                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                    â”‚  EnsembleRetriever  â”‚                                  â”‚
â”‚                    â”‚   (40% BM25 +       â”‚                                  â”‚
â”‚                    â”‚    60% Vectoriel)   â”‚                                  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                               â”‚                                             â”‚
â”‚                               â–¼                                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                    â”‚      LLM (Ollama)   â”‚                                  â”‚
â”‚                    â”‚   llama3.1:8b       â”‚                                  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                               â”‚                                             â”‚
â”‚                               â–¼                                             â”‚
â”‚                         ğŸ’¬ RÃ©ponse                                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Structure des fichiers

```
lib/rag/
â”œâ”€â”€ config.py              # Configuration globale (modÃ¨les, chemins, paramÃ¨tres)
â”œâ”€â”€ main.py                # API FastAPI (endpoints /chat, /sessions)
â”œâ”€â”€ database.py            # Connexion PostgreSQL (SQLAlchemy)
â”œâ”€â”€ models.py              # ModÃ¨les DB (ChatSession, ChatMessage)
â”œâ”€â”€ schemas.py             # SchÃ©mas Pydantic pour l'API
â”œâ”€â”€ data/                  # ğŸ“‚ Placez vos documents ici (PDF, TXT)
â””â”€â”€ rag_engine/
    â”œâ”€â”€ service.py         # Point d'entrÃ©e du systÃ¨me RAG
    â”œâ”€â”€ loader.py          # Chargement des documents via le Router
    â”œâ”€â”€ router.py          # Routage intelligent vers le bon pipeline
    â”œâ”€â”€ vector_store.py    # Gestion des stores (Chroma, DocStore, Retrievers)
    â”œâ”€â”€ chain.py           # CrÃ©ation de la chaÃ®ne LangChain
    â”œâ”€â”€ reranker.py        # Compresseur BGE pour le reranking
    â””â”€â”€ pipelines/
        â”œâ”€â”€ base.py        # Interface de base pour les pipelines
        â”œâ”€â”€ text_pipeline.py   # Pipeline pour documents textuels
        â””â”€â”€ vision_pipeline.py # Pipeline pour documents complexes (LlamaParse)
```

---

## ğŸ”§ Composants techniques

### 1. Ingestion Intelligente (Pipelines)

Le systÃ¨me analyse chaque document et choisit automatiquement le meilleur pipeline de traitement.

| Pipeline | Usage | Technologie |
|----------|-------|-------------|
| **Vision** | PDF complexes (tables, images, mise en page) | LlamaParse (API Cloud) |
| **Texte** | Fichiers texte, PDF simples | PyPDFLoader + Chunking rÃ©cursif |

**Router** : Composant logiciel qui analyse les mÃ©tadonnÃ©es d'un fichier pour diriger son traitement vers le pipeline appropriÃ©.

### 2. Indexation Parent-Child

Cette stratÃ©gie permet d'avoir le meilleur des deux mondes :
- **Recherche prÃ©cise** : Les petits chunks (enfants, 400 tokens) permettent une correspondance fine avec la requÃªte.
- **Contexte riche** : Les documents complets (parents) sont retournÃ©s au LLM pour une rÃ©ponse de qualitÃ©.

| Store | Contenu | Format |
|-------|---------|--------|
| **VectorStore (Chroma)** | Chunks enfants vectorisÃ©s | Vecteurs (Embeddings) |
| **DocStore (LocalFileStore)** | Documents parents complets | Pickle (sÃ©rialisÃ©) |

**Embedding** : Processus de conversion d'un texte en un vecteur numÃ©rique de dimension fixe, capturant son sens sÃ©mantique.

### 3. Recherche Hybride

La recherche combine deux approches complÃ©mentaires :

| MÃ©thode | Force | Poids |
|---------|-------|-------|
| **BM25** | Correspondance exacte de mots-clÃ©s (TF-IDF amÃ©liorÃ©) | 40% |
| **Vectoriel** | ComprÃ©hension sÃ©mantique (sens proche) | 60% |

**BM25** : Algorithme probabiliste de recherche d'information basÃ© sur la frÃ©quence des mots.

### 4. Reranking

AprÃ¨s la recherche initiale, un modÃ¨le de Deep Learning rÃ©Ã©value et rÃ©ordonne les rÃ©sultats.

- **ModÃ¨le** : `BAAI/bge-reranker-v2-m3`
- **Processus** : Les chunks enfants sont rerankÃ©s, puis les parents uniques correspondants sont rÃ©cupÃ©rÃ©s.

**Reranker** : ModÃ¨le spÃ©cialisÃ© qui attribue un score de pertinence Ã  chaque paire (requÃªte, document).

### 5. Caching & Optimisation

| Cache | UtilitÃ© | Stockage |
|-------|---------|----------|
| **LLM Cache** | Ã‰vite de rappeler le LLM pour des questions identiques | SQLite (`cache/llm_cache.db`) |
| **Embeddings Cache** | Ã‰vite de recalculer les vecteurs dÃ©jÃ  connus | Fichiers (`cache/embeddings_cache/`) |

---

## âš™ï¸ Configuration (`config.py`)

```python
# ModÃ¨les
LLM_MODEL = "llama3.1:8b"          # ModÃ¨le de gÃ©nÃ©ration (Ollama)
EMBEDDING_MODEL = "nomic-embed-text" # ModÃ¨le d'embeddings (Ollama)
RERANKER_MODEL = "BAAI/bge-reranker-v2-m3" # ModÃ¨le de reranking (HuggingFace)

# Chunking
CHUNK_SIZE = 1000        # Taille des Parents
CHILD_CHUNK_SIZE = 400   # Taille des Enfants (recherche vectorielle)

# Recherche
SEARCH_K = 10            # Nombre de rÃ©sultats Ã  retourner
USE_RERANKER = True      # Activer le reranking BGE
USE_HYBRID_SEARCH = True # Activer BM25 + Vectoriel
```

---

## ğŸš€ API Endpoints

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `POST` | `/chat` | Envoyer une question et recevoir une rÃ©ponse |
| `GET` | `/sessions` | Liste des conversations (triÃ©es par Ã©pinglage puis date) |
| `GET` | `/sessions/{id}/messages` | Messages d'une conversation |
| `DELETE` | `/sessions/{id}` | Supprimer une conversation |
| `PATCH` | `/sessions/{id}/pin` | Ã‰pingler/dÃ©sÃ©pingler une conversation |

### Exemple de requÃªte `/chat`

```json
{
  "question": "Quel est le risque de ce fonds ?",
  "history": [
    {"role": "user", "content": "Bonjour"},
    {"role": "assistant", "content": "Bonjour ! Comment puis-je vous aider ?"}
  ],
  "session_id": 1
}
```

---

## ğŸ“Š Glossaire technique

| Terme | DÃ©finition |
|-------|------------|
| **RAG** | Retrieval-Augmented Generation : Technique d'IA qui amÃ©liore les rÃ©ponses d'un LLM en lui fournissant des informations pertinentes rÃ©cupÃ©rÃ©es dans une base de connaissances externe. |
| **LLM** | Large Language Model : ModÃ¨le de langage de grande taille capable de gÃ©nÃ©rer du texte. |
| **Chunking** | DÃ©coupage d'un long texte en segments plus courts pour faciliter leur indexation. |
| **Vector Store** | Base de donnÃ©es optimisÃ©e pour stocker et rechercher des vecteurs. |
| **DocStore** | SystÃ¨me de stockage clÃ©-valeur conservant les documents originaux complets. |
| **Retriever** | Composant chargÃ© de retrouver les documents les plus pertinents. |
| **Pipeline** | ChaÃ®ne de traitement sÃ©quentielle (Chargement â†’ Nettoyage â†’ DÃ©coupage). |
| **Streaming** | Mode de transmission oÃ¹ la rÃ©ponse est envoyÃ©e progressivement (token par token). |

---

## ğŸ”„ Flux de traitement d'une question

```
1. Question utilisateur
       â”‚
       â–¼
2. Reformulation (si historique de conversation)
       â”‚
       â–¼
3. Recherche hybride (BM25 + Vectoriel)
       â”‚
       â–¼
4. Reranking des chunks enfants
       â”‚
       â–¼
5. RÃ©cupÃ©ration des documents parents
       â”‚
       â–¼
6. GÃ©nÃ©ration de la rÃ©ponse par le LLM
       â”‚
       â–¼
7. Sauvegarde en base de donnÃ©es
       â”‚
       â–¼
8. RÃ©ponse Ã  l'utilisateur
```

---

## ğŸ› ï¸ PrÃ©requis

### 1. Installation d'Ollama

Ollama permet d'exÃ©cuter des modÃ¨les de langage localement sur votre machine.

#### macOS (avec Homebrew)
```bash
brew install ollama
```

#### Autres systÃ¨mes
TÃ©lÃ©chargez Ollama depuis [ollama.ai](https://ollama.ai) et suivez les instructions d'installation.

### 2. Installation des modÃ¨les

AprÃ¨s avoir installÃ© Ollama, tÃ©lÃ©chargez les modÃ¨les nÃ©cessaires :

```bash
ollama pull llama3.1:8b
ollama pull nomic-embed-text
```

### 3. DÃ©marrage du serveur Ollama

Lancez le serveur Ollama :

```bash
ollama serve
```

### 4. Docker (PostgreSQL)

Pour le stockage des sessions de chat :

```bash
docker-compose up -d
```

### 5. Installation des dÃ©pendances Python

```bash
pip install langchain-ollama langchain-chroma langchain-community pypdf llama-parse python-dotenv fastapi uvicorn sqlalchemy psycopg2-binary sentence-transformers
```

### 6. Configuration LlamaParse (Optionnel)

Pour une meilleure lecture des PDF (tableaux, mises en page complexes), crÃ©ez un fichier `.env` :

```bash
LLAMA_CLOUD_API_KEY=votre_clÃ©_api
```

Obtenez une clÃ© API gratuite sur [LlamaCloud](https://cloud.llamaindex.ai/).

---

## ğŸ“ Utilisation

1. Placez vos documents dans `lib/rag/data/`
2. Lancez l'application : `./run_app.sh`
3. L'indexation se fait automatiquement au premier dÃ©marrage
4. Posez vos questions via l'interface Flutter ou directement via l'API