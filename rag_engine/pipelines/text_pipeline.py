import os
from typing import List
from langchain_core.documents import Document
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from .base import BasePipeline
from config import CHUNK_SIZE, CHUNK_OVERLAP

class TextPipeline(BasePipeline):
    """
    Pipeline B : 'The Fast Lane' (Documents Textuels)
    Utilise des loaders standards et un chunking s√©mantique/r√©cursif.
    
    Chunking: Processus de d√©coupage d'un long texte en segments plus courts ("chunks") pour faciliter leur traitement et leur indexation par le mod√®le.
    """
    
    def process(self, file_path: str) -> List[Document]:
        import time
        start_time = time.time()
        print(f"üèéÔ∏è  Pipeline Texte activ√© pour : {os.path.basename(file_path)}")
        documents = []
        
        try:
            if file_path.lower().endswith(".txt"):
                loader = TextLoader(file_path)
                documents = loader.load()
            elif file_path.lower().endswith(".pdf"):
                print("   ‚Ü≥ Chargement du PDF avec PyPDFLoader...")
                load_start = time.time()
                # Utiliser pypdf directement pour plus de contr√¥le
                from pypdf import PdfReader
                reader = PdfReader(file_path)
                print(f"   ‚Ü≥ PDF ouvert, {len(reader.pages)} pages d√©tect√©es.")
                
                # Charger les pages par lots pour √©viter la surcharge m√©moire
                batch_size = 100
                all_text = ""
                for i in range(0, len(reader.pages), batch_size):
                    batch_end = min(i + batch_size, len(reader.pages))
                    print(f"   ‚Ü≥ Traitement des pages {i+1} √† {batch_end}...")
                    batch_start = time.time()
                    for page_num in range(i, batch_end):
                        page = reader.pages[page_num]
                        text = page.extract_text()
                        if text.strip():
                            all_text += f"\n\n--- Page {page_num + 1} ---\n\n{text}"
                    batch_end_time = time.time()
                    print(f"   ‚Ü≥ Lot trait√© en {batch_end_time - batch_start:.2f}s")
                
                # Cr√©er un document unique avec tout le texte
                from langchain_core.documents import Document
                documents = [Document(page_content=all_text, metadata={"source": file_path})]
                load_end = time.time()
                print(f"   ‚Ü≥ PDF charg√© en {load_end - load_start:.2f}s, 1 document cr√©√©.")
            
            # Chunking
            print("   ‚Ü≥ D√©coupage en chunks...")
            chunk_start = time.time()
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP,
                separators=["\n\n", "\n", ". ", " ", ""] 
            )
            
            splits = text_splitter.split_documents(documents)
            chunk_end = time.time()
            print(f"   ‚Ü≥ D√©coupage termin√© en {chunk_end - chunk_start:.2f}s, {len(splits)} fragments g√©n√©r√©s.")
            
            total_time = time.time() - start_time
            print(f"   ‚Ü≥ Pipeline Texte termin√© en {total_time:.2f}s total.")
            return splits
            
        except Exception as e:
            print(f"‚ùå Erreur dans le pipeline texte : {e}")
            return []
